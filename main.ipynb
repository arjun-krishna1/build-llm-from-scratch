{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6473ac6-5d51-48ac-ba0a-40832bba3f86",
   "metadata": {},
   "source": [
    "# Build a Large language Model from scratch y sebastian Raschka\n",
    "## Chapter 1\n",
    "- LLMs have upended natural language processing\n",
    "- use new deep learning approaches over explicit rules and statistical methods\n",
    "- better at understanding, generating and translating human language\n",
    "- modern llms are first pretrained on a large body of unabeled text to predict the next word in a sentence as a label\n",
    "- then fine-tuned on a smaller, labeled target dataset to follow instructions or perform classification tasks\n",
    "- llms are based on the transformer architecture\n",
    "- the attentoin mechanism gives the llm ability to \"focus\" on specific parts of the input while making one output at a time\n",
    "- datasets of billions of words are needed for llm pretraining\n",
    "- while only trainedto predect next words\n",
    "- emergent properties to classify translate or summarize texts happen too\n",
    "- once pretrained the result is a foundation model\n",
    "- this can be finetuned to be more efficient for different downstream tasks\n",
    "- llms finetuned for specific tasks can outperform general purpose llms on specific tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b67d0-1da1-4663-9934-f882371b3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
